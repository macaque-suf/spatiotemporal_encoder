import torch
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn.decomposition import PCA
import seaborn as sns
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.animation as animation
from typing import List, Tuple, Dict, Optional
import os

from encoders.nerf_spatio_temporal_encoding import NeRFSpatiotemporalEncoding

class SpatiotemporalVisualizer:
    """
    A class to visualize spatiotemporal encodings generated by the SpatiotemporalEncoding model.
    """
    
    def __init__(
        self, 
        embedding_dim: int = 64,
        max_spatial_distance: float = 100.0, 
        max_time: int = 100,
        output_dir: str = "visualizations",
        model_path: Optional[str] = None
    ):
        """
        Initialize the visualizer with model parameters.
        
        Args:
            embedding_dim: Dimension of the embedding space
            max_spatial_distance: Maximum spatial distance used for normalization
            max_time: Maximum time steps used for normalization
            output_dir: Directory where visualizations will be saved
            model_path: Path to the trained encoder model weights
        """
        self.embedding_dim = embedding_dim
        self.max_spatial_distance = max_spatial_distance
        self.max_time = max_time
        self.output_dir = output_dir
        
        # Create output directory if it doesn't exist
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        # Initialize the encoding model
        self.encoder = NeRFSpatiotemporalEncoding(embedding_dim=embedding_dim)
        
        # Load trained weights if path is provided
        if model_path:
            print(f"Loading trained model from {model_path}")
            try:
                # Load state dict
                state_dict = torch.load(model_path)
                self.encoder.load_state_dict(state_dict)
                print("Model loaded successfully!")
            except Exception as e:
                print(f"Error loading model: {e}")
                print("Proceeding with untrained model...")
        
        # Set to evaluation mode
        self.encoder.eval()
        
        # Set random seed for reproducibility
        torch.manual_seed(42)
        np.random.seed(42)
        
        # Set color maps for visualization
        self.cmap = plt.cm.viridis
        self.cmap_time = plt.cm.plasma    

    def visualize_trajectory_analysis(self, model_path=None, num_steps=50, output_file=None):
        """
        Generate a trajectory trace visualization to test spatiotemporal encoding efficacy.
        
        This visualization shows how the encoder represents a single cell moving through
        space over time. It specifically tests the model's ability to integrate both
        spatial and temporal components.
        
        Args:
            model_path: Path to the trained encoder model weights
            num_steps: Number of steps in the trajectory
            output_file: Path to save the visualization
        """
        # Load the trained model if provided
        if model_path:
            print(f"Loading trained model from {model_path}")
            try:
                # Load state dict
                state_dict = torch.load(model_path)
                self.encoder.load_state_dict(state_dict)
                print("Model loaded successfully!")
            except Exception as e:
                print(f"Error loading model: {e}")
                print("Proceeding with untrained model...")
                
        # Set to evaluation mode
        self.encoder.eval()
        
        # Create a trajectory (e.g., a spiral)
        theta = np.linspace(0, 4*np.pi, num_steps)
        radius = np.linspace(10, 40, num_steps)
        
        # Cell positions
        x = radius * np.cos(theta) + self.max_spatial_distance/2
        y = radius * np.sin(theta) + self.max_spatial_distance/2
        positions = np.column_stack([x, y])
        
        # Convert to tensor
        positions_tensor = torch.tensor(positions, dtype=torch.float32).unsqueeze(0)
        
        # Create time points (increasing along trajectory)
        time_tensor = torch.linspace(0, self.max_time, num_steps).reshape(1, num_steps, 1)
        
        # Baseline features (zeros)
        cell_features = torch.zeros(1, num_steps, self.embedding_dim)
        
        # Generate encodings
        with torch.no_grad():
            encodings = self.encoder(positions_tensor, time_tensor, cell_features)
        
        # Perform PCA
        pca = PCA(n_components=3)
        pca_result = pca.fit_transform(encodings.squeeze(0).numpy())
        
        # Create 3D figure
        fig = plt.figure(figsize=(15, 10))
        
        # Physical space + time subplot
        ax1 = fig.add_subplot(121, projection='3d')
        scatter1 = ax1.scatter(positions[:, 0], positions[:, 1], time_tensor.squeeze().numpy(), 
                             c=time_tensor.squeeze().numpy(), cmap=self.cmap_time, alpha=0.8)
        ax1.set_title('Cell Trajectory in Physical Space + Time')
        ax1.set_xlabel('X Position')
        ax1.set_ylabel('Y Position')
        ax1.set_zlabel('Time')
        plt.colorbar(scatter1, ax=ax1, label='Time')
        
        # Add trajectory line
        ax1.plot(positions[:, 0], positions[:, 1], time_tensor.squeeze().numpy(), 'k-', alpha=0.4)
        
        # Encoding space subplot
        ax2 = fig.add_subplot(122, projection='3d')
        scatter2 = ax2.scatter(pca_result[:, 0], pca_result[:, 1], pca_result[:, 2], 
                             c=time_tensor.squeeze().numpy(), cmap=self.cmap_time, alpha=0.8)
        ax2.set_title('Cell Trajectory in Encoding Space (PCA)')
        ax2.set_xlabel('PC1')
        ax2.set_ylabel('PC2')
        ax2.set_zlabel('PC3')
        plt.colorbar(scatter2, ax=ax2, label='Time')
        
        # Add trajectory line in encoding space
        ax2.plot(pca_result[:, 0], pca_result[:, 1], pca_result[:, 2], 'k-', alpha=0.4)
        
        plt.tight_layout()
        
        # Save figure
        if output_file is None:
            output_file = os.path.join(self.output_dir, "cell_trajectory.png")
        plt.savefig(output_file, dpi=300)
        plt.close()
        
        print(f"Trajectory analysis visualization saved to {output_file}")
        
        return pca_result

    def generate_pattern(
        self, 
        pattern_type: str,
        num_cells: int = 100,
        time_points: List[int] = [0, 25, 50, 75, 100],
        grid_size: int = 10,
        spiral_loops: float = 3.0,
        cluster_centers: int = 4,
        cluster_std: float = 10.0,
        seed: int = 42
    ) -> Dict[str, torch.Tensor]:
        """
        Generate different patterns of spatial positions.
        
        Args:
            pattern_type: Type of pattern ('grid', 'spiral', 'random', 'clusters')
            num_cells: Number of cells to generate
            time_points: List of time points to generate data for
            grid_size: Size of the grid for 'grid' pattern
            spiral_loops: Number of loops for 'spiral' pattern
            cluster_centers: Number of cluster centers for 'clusters' pattern
            cluster_std: Standard deviation of clusters
            seed: Random seed
            
        Returns:
            Dictionary with positions, time_points, and embeddings for each pattern
        """
        np.random.seed(seed)
        torch.manual_seed(seed)
        
        # Create empty dictionaries to store results
        results = {
            'positions': [],
            'time_values': [],
            'encodings': [],
            'time_points': time_points
        }
        
        # Generate positions based on pattern type
        if pattern_type == 'grid':
            # Generate grid pattern
            side = int(np.sqrt(num_cells))
            x = np.linspace(0, self.max_spatial_distance, side)
            y = np.linspace(0, self.max_spatial_distance, side)
            xx, yy = np.meshgrid(x, y)
            positions = np.column_stack([xx.flatten(), yy.flatten()])
            positions = positions[:num_cells]  # Ensure we have exactly num_cells
            
        elif pattern_type == 'spiral':
            # Generate spiral pattern
            theta = np.linspace(0, 2 * np.pi * spiral_loops, num_cells)
            radius = np.linspace(1, self.max_spatial_distance / 2, num_cells)
            x = radius * np.cos(theta) + self.max_spatial_distance / 2
            y = radius * np.sin(theta) + self.max_spatial_distance / 2
            positions = np.column_stack([x, y])
            
        elif pattern_type == 'random':
            # Generate random positions
            positions = np.random.uniform(0, self.max_spatial_distance, (num_cells, 2))
            
        elif pattern_type == 'clusters':
            # Generate clustered positions
            centers = np.random.uniform(20, self.max_spatial_distance - 20, (cluster_centers, 2))
            cluster_assignments = np.random.choice(cluster_centers, num_cells)
            positions = centers[cluster_assignments] + np.random.normal(0, cluster_std, (num_cells, 2))
            # Ensure positions are within bounds
            positions = np.clip(positions, 0, self.max_spatial_distance)
        else:
            raise ValueError(f"Unknown pattern type: {pattern_type}")
        
        # Convert positions to tensor
        positions_tensor = torch.tensor(positions, dtype=torch.float32).unsqueeze(0)  # [1, num_cells, 2]
        
        # Generate cell features (random for now, could be based on positions in real scenarios)
        cell_features = torch.randn(1, num_cells, self.embedding_dim)
        
        # Process each time point
        for t in time_points:
            time_tensor = torch.ones(1, num_cells, 1) * t
            
            # Generate embeddings
            with torch.no_grad():
                encodings = self.encoder(
                    positions=positions_tensor,
                    time_points=time_tensor,
                    cell_features=cell_features
                )
            
            results['positions'].append(positions_tensor.clone())
            results['time_values'].append(time_tensor.clone())
            results['encodings'].append(encodings.clone())
        
        return results
    
    def visualize_encoding_space(
        self, 
        results: Dict[str, List[torch.Tensor]],
        pattern_type: str,
        plot_type: str = '2d',
        save_path: Optional[str] = None
    ):
        """
        Visualize the encoding space using PCA.
        
        Args:
            results: Dictionary containing positions, time_values, and encodings
            pattern_type: Type of pattern for naming files
            plot_type: '2d' or '3d' for the type of plot
            save_path: Path to save the visualization, if None use default
        """
        positions = results['positions']
        encodings = results['encodings']
        time_points = results['time_points']
        
        # Flatten encodings for all time points
        all_encodings = torch.cat(encodings, dim=1).squeeze(0).numpy()
        all_positions = torch.cat(positions, dim=1).squeeze(0).numpy()
        
        # Create time labels for coloring
        time_labels = []
        for t_idx, t in enumerate(time_points):
            time_labels.extend([t] * positions[0].shape[1])
        time_labels = np.array(time_labels)
        
        # Normalize time labels for coloring
        norm_time_labels = time_labels / self.max_time
        
        # Perform PCA
        pca = PCA(n_components=3)
        pca_result = pca.fit_transform(all_encodings)
        
        # Create figure
        plt.figure(figsize=(15, 12))
        
        if plot_type == '2d':
            # Create 2D plots
            plt.subplot(2, 2, 1)
            plt.scatter(all_positions[:, 0], all_positions[:, 1], c=time_labels, cmap=self.cmap_time, alpha=0.7)
            plt.colorbar(label='Time')
            plt.title(f'{pattern_type.capitalize()} Pattern - Physical Space')
            plt.xlabel('X Position')
            plt.ylabel('Y Position')
            
            plt.subplot(2, 2, 2)
            plt.scatter(pca_result[:, 0], pca_result[:, 1], c=time_labels, cmap=self.cmap_time, alpha=0.7)
            plt.colorbar(label='Time')
            plt.title(f'{pattern_type.capitalize()} Pattern - Encoding Space (PCA)')
            plt.xlabel('PC1')
            plt.ylabel('PC2')
            
            # Show embedding trajectories for selected cells
            plt.subplot(2, 2, 3)
            num_cells = positions[0].shape[1]
            selected_cells = np.linspace(0, num_cells-1, 5, dtype=int)
            
            for cell_idx in selected_cells:
                cell_pca = []
                for t_idx in range(len(time_points)):
                    cell_encoding = encodings[t_idx][0, cell_idx].numpy()
                    cell_pca.append(pca.transform([cell_encoding])[0])
                cell_pca = np.array(cell_pca)
                plt.plot(cell_pca[:, 0], cell_pca[:, 1], 'o-', label=f'Cell {cell_idx}')
            
            plt.title('Embedding Trajectories for Selected Cells')
            plt.xlabel('PC1')
            plt.ylabel('PC2')
            plt.legend()
            
            # Plot explained variance
            plt.subplot(2, 2, 4)
            explained_var = pca.explained_variance_ratio_
            plt.bar(range(1, 4), explained_var[:3])
            plt.xlabel('Principal Component')
            plt.ylabel('Explained Variance Ratio')
            plt.title('Explained Variance by Principal Components')
            plt.xticks([1, 2, 3])
            
        elif plot_type == '3d':
            # Create 3D plots
            ax1 = plt.subplot(1, 2, 1, projection='3d')
            scatter1 = ax1.scatter(
                all_positions[:, 0], 
                all_positions[:, 1], 
                time_labels, 
                c=time_labels, 
                cmap=self.cmap_time, 
                alpha=0.7
            )
            ax1.set_title(f'{pattern_type.capitalize()} Pattern - Physical Space + Time')
            ax1.set_xlabel('X Position')
            ax1.set_ylabel('Y Position')
            ax1.set_zlabel('Time')
            plt.colorbar(scatter1, ax=ax1, label='Time')
            
            ax2 = plt.subplot(1, 2, 2, projection='3d')
            scatter2 = ax2.scatter(
                pca_result[:, 0], 
                pca_result[:, 1], 
                pca_result[:, 2], 
                c=time_labels, 
                cmap=self.cmap_time, 
                alpha=0.7
            )
            ax2.set_title(f'{pattern_type.capitalize()} Pattern - Encoding Space (PCA)')
            ax2.set_xlabel('PC1')
            ax2.set_ylabel('PC2')
            ax2.set_zlabel('PC3')
            plt.colorbar(scatter2, ax=ax2, label='Time')
        
        plt.tight_layout()
        
        # Save figure
        if save_path is None:
            save_path = os.path.join(self.output_dir, f"{pattern_type}_{plot_type}.png")
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
    
    def create_animation(
        self, 
        results: Dict[str, List[torch.Tensor]],
        pattern_type: str,
        save_path: Optional[str] = None
    ):
        """
        Create an animation showing how the encoding space evolves with time.
        
        Args:
            results: Dictionary containing positions, time_values, and encodings
            pattern_type: Type of pattern for naming files
            save_path: Path to save the animation, if None use default
        """
        positions = results['positions']
        encodings = results['encodings']
        time_points = results['time_points']
        
        # Project all encodings to 2D using PCA for consistency
        all_encodings = torch.cat(encodings, dim=1).squeeze(0).numpy()
        pca = PCA(n_components=2)
        pca.fit(all_encodings)
        
        # Set up the figure
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))
        fig.suptitle(f'{pattern_type.capitalize()} Pattern - Temporal Evolution', fontsize=16)
        
        def update(frame):
            # Clear the axes
            ax1.clear()
            ax2.clear()
            
            # Get data for this time point
            pos = positions[frame].squeeze(0).numpy()
            enc = encodings[frame].squeeze(0).numpy()
            time_value = time_points[frame]
            
            # Project encodings to 2D
            enc_2d = pca.transform(enc)
            
            # Plot physical space
            scatter1 = ax1.scatter(pos[:, 0], pos[:, 1], c=np.ones(len(pos)) * time_value, 
                                  cmap=self.cmap_time, vmin=0, vmax=self.max_time)
            ax1.set_title(f'Physical Space (t={time_value})')
            ax1.set_xlabel('X Position')
            ax1.set_ylabel('Y Position')
            ax1.set_xlim(0, self.max_spatial_distance)
            ax1.set_ylim(0, self.max_spatial_distance)
            
            # Plot encoding space
            scatter2 = ax2.scatter(enc_2d[:, 0], enc_2d[:, 1], c=np.ones(len(enc)) * time_value,
                                  cmap=self.cmap_time, vmin=0, vmax=self.max_time)
            ax2.set_title(f'Encoding Space (PCA, t={time_value})')
            ax2.set_xlabel('PC1')
            ax2.set_ylabel('PC2')
            
            # Add colorbar only once
            if frame == 0:
                fig.colorbar(scatter1, ax=[ax1, ax2], label='Time')
            
            return scatter1, scatter2
        
        # Create animation
        ani = animation.FuncAnimation(
            fig, update, frames=len(time_points), blit=False, repeat=True
        )
        
        # Save animation
        if save_path is None:
            save_path = os.path.join(self.output_dir, f"{pattern_type}_animation.gif")
        
        ani.save(save_path, writer='pillow', fps=2, dpi=150)
        plt.close()
    
    def generate_morphospace(
        self,
        patterns: List[str] = ['grid', 'spiral', 'random', 'clusters'],
        num_cells: int = 100,
        time_points: List[int] = [0, 25, 50, 75, 100],
        save_path: Optional[str] = None
    ):
        """
        Generate a unified morphospace visualization showing how different patterns map into the encoding space.
        
        Args:
            patterns: List of patterns to include
            num_cells: Number of cells for each pattern
            time_points: List of time points
            save_path: Path to save the visualization
        """
        # Generate data for each pattern
        all_pattern_results = {}
        for pattern in patterns:
            all_pattern_results[pattern] = self.generate_pattern(
                pattern_type=pattern,
                num_cells=num_cells,
                time_points=time_points
            )
        
        # Collect all encodings
        all_encodings = []
        pattern_labels = []
        time_labels = []
        
        for pattern, results in all_pattern_results.items():
            for t_idx, t in enumerate(time_points):
                encodings = results['encodings'][t_idx].squeeze(0).numpy()
                all_encodings.append(encodings)
                pattern_labels.extend([pattern] * encodings.shape[0])
                time_labels.extend([t] * encodings.shape[0])
        
        all_encodings = np.vstack(all_encodings)
        pattern_labels = np.array(pattern_labels)
        time_labels = np.array(time_labels)
        
        # Apply PCA to all encodings
        pca = PCA(n_components=3)
        pca_result = pca.fit_transform(all_encodings)
        
        # Create figure for pattern comparison
        plt.figure(figsize=(15, 12))
        
        # Plot by pattern
        ax1 = plt.subplot(2, 2, 1)
        for pattern in patterns:
            mask = pattern_labels == pattern
            plt.scatter(
                pca_result[mask, 0], 
                pca_result[mask, 1], 
                alpha=0.7, 
                label=pattern
            )
        plt.title('Morphospace by Pattern Type')
        plt.xlabel('PC1')
        plt.ylabel('PC2')
        plt.legend()
        
        # Plot by time
        ax2 = plt.subplot(2, 2, 2)
        scatter = plt.scatter(
            pca_result[:, 0], 
            pca_result[:, 1], 
            c=time_labels, 
            cmap=self.cmap_time, 
            alpha=0.7
        )
        plt.colorbar(scatter, label='Time')
        plt.title('Morphospace by Time')
        plt.xlabel('PC1')
        plt.ylabel('PC2')
        
        # Plot 3D
        ax3 = plt.subplot(2, 2, 3, projection='3d')
        for pattern in patterns:
            mask = pattern_labels == pattern
            ax3.scatter(
                pca_result[mask, 0], 
                pca_result[mask, 1], 
                pca_result[mask, 2], 
                alpha=0.5, 
                label=pattern
            )
        ax3.set_title('3D Morphospace by Pattern Type')
        ax3.set_xlabel('PC1')
        ax3.set_ylabel('PC2')
        ax3.set_zlabel('PC3')
        ax3.legend()
        
        # Plot explained variance
        ax4 = plt.subplot(2, 2, 4)
        explained_var = pca.explained_variance_ratio_
        plt.bar(range(1, 4), explained_var[:3])
        plt.xlabel('Principal Component')
        plt.ylabel('Explained Variance Ratio')
        plt.title('Explained Variance by Principal Components')
        plt.xticks([1, 2, 3])
        
        plt.tight_layout()
        
        # Save figure
        if save_path is None:
            save_path = os.path.join(self.output_dir, "unified_morphospace.png")
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()


    def visualize_boundary_response(self, model_path=None, resolution=20, output_file=None):
        """
        Test how the encoder handles sharp spatial boundaries with constant time.
        
        Creates a grid with distinct regions (circular clusters) and visualizes 
        how these boundaries are preserved in the encoding space.
        
        Args:
            model_path: Path to the trained encoder model weights
            resolution: Resolution of the grid
            output_file: Path to save the visualization
        """
        # Load model if provided
        if model_path:
            print(f"Loading trained model from {model_path}")
            try:
                state_dict = torch.load(model_path)
                self.encoder.load_state_dict(state_dict)
                print("Model loaded successfully!")
            except Exception as e:
                print(f"Error loading model: {e}")
                print("Proceeding with untrained model...")
        
        # Set to evaluation mode
        self.encoder.eval()
        
        # Create a grid of positions
        x = np.linspace(0, self.max_spatial_distance, resolution)
        y = np.linspace(0, self.max_spatial_distance, resolution)
        xx, yy = np.meshgrid(x, y)
        positions = np.column_stack([xx.flatten(), yy.flatten()])
        
        # Create cluster boundaries (3 circular regions)
        cluster_centers = [
            [30, 30],  # Bottom left
            [70, 30],  # Bottom right
            [50, 70]   # Top middle
        ]
        cluster_radii = [15, 20, 25]
        
        # Assign cluster identity to each position
        cluster_ids = np.zeros(len(positions))
        
        for i, (center, radius) in enumerate(zip(cluster_centers, cluster_radii)):
            # Calculate distance from center
            distances = np.sqrt(np.sum((positions - center) ** 2, axis=1))
            
            # Points within radius belong to this cluster
            cluster_ids[distances < radius] = i + 1
        
        # Convert to tensors
        positions_tensor = torch.tensor(positions, dtype=torch.float32).unsqueeze(0)
        
        # Fixed time (middle of range)
        time_tensor = torch.ones(1, len(positions), 1) * (self.max_time / 2)
        
        # Baseline features
        cell_features = torch.zeros(1, len(positions), self.embedding_dim)
        
        # Generate encodings
        with torch.no_grad():
            encodings = self.encoder(positions_tensor, time_tensor, cell_features)
        
        # Perform PCA
        pca = PCA(n_components=3)
        pca_result = pca.fit_transform(encodings.squeeze(0).numpy())
        
        # Create figure
        fig = plt.figure(figsize=(15, 10))
        
        # Physical space
        ax1 = fig.add_subplot(121)
        scatter1 = ax1.scatter(positions[:, 0], positions[:, 1], c=cluster_ids, 
                            cmap='viridis', s=30, alpha=0.8)
        ax1.set_title('Cell Clusters in Physical Space')
        ax1.set_xlabel('X Position')
        ax1.set_ylabel('Y Position')
        plt.colorbar(scatter1, ax=ax1, label='Cluster ID')
        
        # Mark cluster centers
        for center in cluster_centers:
            ax1.plot(center[0], center[1], 'ro', markersize=10)
        
        # Encoding space
        ax2 = fig.add_subplot(122, projection='3d')
        scatter2 = ax2.scatter(pca_result[:, 0], pca_result[:, 1], pca_result[:, 2], 
                            c=cluster_ids, cmap='viridis', s=30, alpha=0.8)
        ax2.set_title('Cluster Boundaries in Encoding Space (PCA)')
        ax2.set_xlabel('PC1')
        ax2.set_ylabel('PC2')
        ax2.set_zlabel('PC3')
        plt.colorbar(scatter2, ax=ax2, label='Cluster ID')
        
        plt.tight_layout()
        
        # Save figure
        if output_file is None:
            output_file = os.path.join(self.output_dir, "boundary_response.png")
        plt.savefig(output_file, dpi=300)
        plt.close()
        
        print(f"Boundary response visualization saved to {output_file}")
        
        return pca_result, cluster_ids

    def visualize_temporal_oscillations(self, model_path=None, num_points=50, output_file=None):
        """
        Test how the encoder captures different temporal frequencies.
        
        Creates a fixed spatial grid of points where each point oscillates at
        different temporal frequencies.
        
        Args:
            model_path: Path to the trained encoder model weights
            num_points: Number of points in the grid
            output_file: Path to save the visualization
        """
        # Load model if provided
        if model_path:
            print(f"Loading trained model from {model_path}")
            try:
                state_dict = torch.load(model_path)
                self.encoder.load_state_dict(state_dict)
                print("Model loaded successfully!")
            except Exception as e:
                print(f"Error loading model: {e}")
                print("Proceeding with untrained model...")
        
        # Set to evaluation mode
        self.encoder.eval()
        
        # Create a grid of positions (fixed spatial locations)
        side = int(np.sqrt(num_points))
        x = np.linspace(20, 80, side)
        y = np.linspace(20, 80, side)
        xx, yy = np.meshgrid(x, y)
        positions = np.column_stack([xx.flatten(), yy.flatten()])
        num_positions = len(positions)
        
        # Convert to tensor and repeat for all time points
        positions_tensor = torch.tensor(positions, dtype=torch.float32).unsqueeze(0)
        
        # Create 5 different temporal frequencies
        num_frequencies = 5
        frequency_multipliers = np.linspace(1, 5, num_frequencies)
        frequencies = frequency_multipliers * (2 * np.pi / self.max_time)
        
        # Assign each point a frequency
        point_frequencies = np.zeros(num_positions)
        for i in range(num_positions):
            point_frequencies[i] = frequencies[i % num_frequencies]
        
        # Create time points
        num_time_steps = 40
        time_points = np.linspace(0, self.max_time, num_time_steps)
        
        # Store all encodings
        all_encodings = []
        oscillation_values = []
        
        # Process each time point
        for t in time_points:
            # Calculate oscillation value at this time for each point
            oscillation = np.sin(point_frequencies * t)
            oscillation_values.append(oscillation)
            
            # Time tensor for this step
            time_tensor = torch.ones(1, num_positions, 1) * t
            
            # Baseline features
            cell_features = torch.zeros(1, num_positions, self.embedding_dim)
            
            # Generate encodings
            with torch.no_grad():
                encodings = self.encoder(positions_tensor, time_tensor, cell_features)
                all_encodings.append(encodings.squeeze(0).numpy())
        
        # Convert to arrays
        all_encodings = np.array(all_encodings)  # [num_time_steps, num_positions, embedding_dim]
        oscillation_values = np.array(oscillation_values)  # [num_time_steps, num_positions]
        
        # Perform PCA on all encodings
        pca = PCA(n_components=3)
        all_encodings_flat = all_encodings.reshape(-1, self.embedding_dim)
        pca_result = pca.fit_transform(all_encodings_flat)
        pca_result = pca_result.reshape(num_time_steps, num_positions, 3)
        
        # Create figure
        fig = plt.figure(figsize=(15, 10))
        
        # Plot for each frequency
        for freq_idx in range(num_frequencies):
            # Find points with this frequency
            point_indices = np.where(point_frequencies == frequencies[freq_idx])[0]
            
            if len(point_indices) == 0:
                continue
                
            # Take the first point for this frequency
            point_idx = point_indices[0]
            
            # Extract oscillation values and encodings for this point
            osc_values = oscillation_values[:, point_idx]
            point_encodings = pca_result[:, point_idx, :]
            
            # Plot oscillation values vs. encoding values
            ax = fig.add_subplot(num_frequencies, 1, freq_idx + 1)
            
            # Plot oscillation values
            ax.plot(time_points, osc_values, 'b-', label=f'Oscillation (f={frequency_multipliers[freq_idx]:.1f}x)')
            
            # Plot first 3 PCA components
            ax.plot(time_points, point_encodings[:, 0], 'r-', label='PCA 1')
            ax.plot(time_points, point_encodings[:, 1], 'g-', label='PCA 2')
            ax.plot(time_points, point_encodings[:, 2], 'y-', label='PCA 3')
            
            ax.set_title(f'Temporal Frequency {frequency_multipliers[freq_idx]:.1f}x Response')
            ax.set_xlabel('Time')
            ax.set_ylabel('Value')
            ax.legend()
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # Save figure
        if output_file is None:
            output_file = os.path.join(self.output_dir, "temporal_oscillations.png")
        plt.savefig(output_file, dpi=300)
        plt.close()
        
        # Create 3D visualization of all point trajectories
        fig = plt.figure(figsize=(15, 10))
        ax = fig.add_subplot(111, projection='3d')
        
        # Plot trajectory for each point (using different colors based on frequency)
        for point_idx in range(min(num_positions, 25)):  # Limit to 25 points for clarity
            freq_idx = point_idx % num_frequencies
            point_encodings = pca_result[:, point_idx, :]
            
            ax.plot(point_encodings[:, 0], point_encodings[:, 1], point_encodings[:, 2], 
                label=f'f={frequency_multipliers[freq_idx]:.1f}x' if point_idx < num_frequencies else None)
        
        ax.set_title('Trajectories of Points with Different Temporal Frequencies in Encoding Space')
        ax.set_xlabel('PC1')
        ax.set_ylabel('PC2')
        ax.set_zlabel('PC3')
        ax.legend()
        
        # Save 3D visualization
        trajectory_file = os.path.join(self.output_dir, "frequency_trajectories.png")
        plt.savefig(trajectory_file, dpi=300)
        plt.close()
        
        print(f"Temporal oscillation visualization saved to {output_file}")
        print(f"Frequency trajectories visualization saved to {trajectory_file}")
        
        return point_frequencies, all_encodings

    def visualize_ablation_comparison(self, trained_model_path, output_file=None):
        """
        Compare trained encoder against baseline sinusoidal encoding.
        
        Creates a side-by-side comparison of the encoding spaces for both models
        on the same input data.
        
        Args:
            trained_model_path: Path to the trained encoder model weights
            output_file: Path to save the visualization
        """
        if trained_model_path is None:
            print("No trained model path provided for ablation comparison")
            return
        
        # Create a baseline sinusoidal encoder with the same parameters
        baseline_encoder = NeRFSpatiotemporalEncoding(embedding_dim=self.embedding_dim)
        
        # Load trained model
        print(f"Loading trained model from {trained_model_path}")
        try:
            state_dict = torch.load(trained_model_path)
            self.encoder.load_state_dict(state_dict)
            print("Trained model loaded successfully!")
        except Exception as e:
            print(f"Error loading trained model: {e}")
            return
        
        # Set both models to evaluation mode
        self.encoder.eval()
        baseline_encoder.eval()
        
        # Create test data - spiral trajectory
        num_steps = 100
        theta = np.linspace(0, 4*np.pi, num_steps)
        radius = np.linspace(10, 40, num_steps)
        
        # Cell positions
        x = radius * np.cos(theta) + self.max_spatial_distance/2
        y = radius * np.sin(theta) + self.max_spatial_distance/2
        positions = np.column_stack([x, y])
        
        # Convert to tensor
        positions_tensor = torch.tensor(positions, dtype=torch.float32).unsqueeze(0)
        
        # Create time points (increasing along trajectory)
        time_tensor = torch.linspace(0, self.max_time, num_steps).reshape(1, num_steps, 1)
        
        # Baseline features (zeros)
        cell_features = torch.zeros(1, num_steps, self.embedding_dim)
        
        # Generate encodings with both models
        with torch.no_grad():
            trained_encodings = self.encoder(positions_tensor, time_tensor, cell_features)
            baseline_encodings = baseline_encoder(positions_tensor, time_tensor, cell_features)
        
        # Perform PCA on all encodings together for consistent comparison
        combined_encodings = torch.cat([
            trained_encodings.squeeze(0),
            baseline_encodings.squeeze(0)
        ], dim=0).numpy()
        
        pca = PCA(n_components=3)
        pca_result = pca.fit_transform(combined_encodings)
        
        # Split back into trained and baseline results
        trained_pca = pca_result[:num_steps]
        baseline_pca = pca_result[num_steps:]
        
        # Create figure
        fig = plt.figure(figsize=(15, 10))
        
        # Baseline model subplot
        ax1 = fig.add_subplot(121, projection='3d')
        scatter1 = ax1.scatter(baseline_pca[:, 0], baseline_pca[:, 1], baseline_pca[:, 2], 
                            c=time_tensor.squeeze().numpy(), cmap=self.cmap_time, alpha=0.8)
        ax1.set_title('Baseline Fixed Sinusoidal Encoding')
        ax1.set_xlabel('PC1')
        ax1.set_ylabel('PC2')
        ax1.set_zlabel('PC3')
        plt.colorbar(scatter1, ax=ax1, label='Time')
        
        # Add trajectory line
        ax1.plot(baseline_pca[:, 0], baseline_pca[:, 1], baseline_pca[:, 2], 'k-', alpha=0.4)
        
        # Trained model subplot
        ax2 = fig.add_subplot(122, projection='3d')
        scatter2 = ax2.scatter(trained_pca[:, 0], trained_pca[:, 1], trained_pca[:, 2], 
                            c=time_tensor.squeeze().numpy(), cmap=self.cmap_time, alpha=0.8)
        ax2.set_title('Trained Model with Learned Projections')
        ax2.set_xlabel('PC1')
        ax2.set_ylabel('PC2')
        ax2.set_zlabel('PC3')
        plt.colorbar(scatter2, ax=ax2, label='Time')
        
        # Add trajectory line
        ax2.plot(trained_pca[:, 0], trained_pca[:, 1], trained_pca[:, 2], 'k-', alpha=0.4)
        
        plt.tight_layout()
        
        # Save figure
        if output_file is None:
            output_file = os.path.join(self.output_dir, "ablation_comparison.png")
        plt.savefig(output_file, dpi=300)
        plt.close()
        
        # Calculate quantitative metrics
        # 1. Variance explained by PCA
        trained_var = np.var(trained_encodings.squeeze(0).numpy(), axis=0).sum()
        baseline_var = np.var(baseline_encodings.squeeze(0).numpy(), axis=0).sum()
        
        # 2. Smoothness of trajectory (average distance between consecutive points)
        trained_smoothness = np.mean(np.sqrt(np.sum(np.diff(trained_pca, axis=0)**2, axis=1)))
        baseline_smoothness = np.mean(np.sqrt(np.sum(np.diff(baseline_pca, axis=0)**2, axis=1)))
        
        # 3. Temporal progression (correlation between time and distance along trajectory)
        trained_dists = np.cumsum(np.sqrt(np.sum(np.diff(trained_pca, axis=0)**2, axis=1)))
        trained_dists = np.insert(trained_dists, 0, 0)  # Add starting point
        
        baseline_dists = np.cumsum(np.sqrt(np.sum(np.diff(baseline_pca, axis=0)**2, axis=1)))
        baseline_dists = np.insert(baseline_dists, 0, 0)  # Add starting point
        
        trained_time_corr = np.corrcoef(trained_dists, time_tensor.squeeze().numpy())[0, 1]
        baseline_time_corr = np.corrcoef(baseline_dists, time_tensor.squeeze().numpy())[0, 1]
        
        # Save metrics to a text file
        metrics_file = os.path.join(self.output_dir, "ablation_metrics.txt")
        with open(metrics_file, 'w') as f:
            f.write("ABLATION COMPARISON METRICS\n")
            f.write("===========================\n\n")
            f.write(f"Total Variance in Encoding Space:\n")
            f.write(f"  Trained Model: {trained_var:.4f}\n")
            f.write(f"  Baseline Model: {baseline_var:.4f}\n")
            f.write(f"  Ratio (Trained/Baseline): {trained_var/baseline_var:.4f}\n\n")
            
            f.write(f"Trajectory Smoothness (avg. distance between points):\n")
            f.write(f"  Trained Model: {trained_smoothness:.4f}\n")
            f.write(f"  Baseline Model: {baseline_smoothness:.4f}\n")
            f.write(f"  Ratio (Trained/Baseline): {trained_smoothness/baseline_smoothness:.4f}\n\n")
            
            f.write(f"Temporal Correlation (time vs. distance along trajectory):\n")
            f.write(f"  Trained Model: {trained_time_corr:.4f}\n")
            f.write(f"  Baseline Model: {baseline_time_corr:.4f}\n")
            f.write(f"  Difference (Trained - Baseline): {trained_time_corr - baseline_time_corr:.4f}\n")
        
        print(f"Ablation comparison visualization saved to {output_file}")
        print(f"Metrics saved to {metrics_file}")
        
        return {
            "trained_variance": trained_var,
            "baseline_variance": baseline_var,
            "trained_smoothness": trained_smoothness,
            "baseline_smoothness": baseline_smoothness,
            "trained_time_correlation": trained_time_corr,
            "baseline_time_correlation": baseline_time_corr
        }

    def visualize_attention_heatmap(self, model_path=None, resolution=50, output_file=None):
        """
        Generate an attention heatmap showing which spatial regions receive
        higher attention in the learned spatial projections.
        
        Args:
            model_path: Path to the trained encoder model weights
            resolution: Resolution of the grid
            output_file: Path to save the visualization
        """
        # Load model if provided
        if model_path:
            print(f"Loading trained model from {model_path}")
            try:
                state_dict = torch.load(model_path)
                self.encoder.load_state_dict(state_dict)
                print("Model loaded successfully!")
            except Exception as e:
                print(f"Error loading model: {e}")
                print("Proceeding with untrained model...")
        
        # Set to evaluation mode
        self.encoder.eval()
        
        # Create a grid of positions
        x = np.linspace(0, self.max_spatial_distance, resolution)
        y = np.linspace(0, self.max_spatial_distance, resolution)
        xx, yy = np.meshgrid(x, y)
        positions = np.column_stack([xx.flatten(), yy.flatten()])
        
        # Convert to tensor
        positions_tensor = torch.tensor(positions, dtype=torch.float32).unsqueeze(0)
        
        # Fixed time (middle of range)
        time_tensor = torch.ones(1, len(positions), 1) * (self.max_time / 2)
        
        # Baseline features
        cell_features = torch.zeros(1, len(positions), self.embedding_dim)
        
        # Record activation magnitudes at different positions
        with torch.no_grad():
            # Forward pass through encoder
            encodings = self.encoder(positions_tensor, time_tensor, cell_features)
            
            # Access spatial encoder activations if possible
            # Note: We need to register hooks or modify the model to expose intermediate activations
            # Here's a workaround - compute the encoder again with a manual hook
            
            # Create a hook to capture activations
            activations = {}
            
            def hook_fn(name):
                def hook(module, input, output):
                    activations[name] = output.detach()
                return hook
            
            # Register hooks for spatial encoders
            if hasattr(self.encoder, 'spatial_encoder_x'):
                # For enhanced encoder
                handle1 = self.encoder.spatial_encoder_x.register_forward_hook(hook_fn('spatial_x'))
                handle2 = self.encoder.spatial_encoder_y.register_forward_hook(hook_fn('spatial_y'))
            
                # Run forward pass again to capture activations
                _ = self.encoder(positions_tensor, time_tensor, cell_features)
                
                # Remove hooks
                handle1.remove()
                handle2.remove()
                
                # Compute activation magnitude (sum of absolute values across dimensions)
                if 'spatial_x' in activations and 'spatial_y' in activations:
                    x_activations = torch.sum(torch.abs(activations['spatial_x']), dim=2).squeeze(0)
                    y_activations = torch.sum(torch.abs(activations['spatial_y']), dim=2).squeeze(0)
                    
                    # Combine x and y activations
                    combined_activations = (x_activations + y_activations).numpy()
                else:
                    # Fallback - just use the encoding magnitude as a proxy
                    combined_activations = torch.sum(torch.abs(encodings - cell_features), dim=2).squeeze(0).numpy()
            else:
                # Fallback for basic encoder - use encoding magnitude
                combined_activations = torch.sum(torch.abs(encodings - cell_features), dim=2).squeeze(0).numpy()
        
        # Reshape to grid
        attention_map = combined_activations.reshape(resolution, resolution)
        
        # Create visualization
        plt.figure(figsize=(12, 10))
        
        # Plot attention heatmap
        plt.imshow(attention_map, extent=[0, self.max_spatial_distance, 0, self.max_spatial_distance], 
                origin='lower', cmap='viridis')
        plt.colorbar(label='Activation Magnitude')
        plt.title('Spatial Attention Heatmap')
        plt.xlabel('X Position')
        plt.ylabel('Y Position')
        
        # Save figure
        if output_file is None:
            output_file = os.path.join(self.output_dir, "attention_heatmap.png")
        plt.savefig(output_file, dpi=300)
        plt.close()
        
        print(f"Attention heatmap saved to {output_file}")
        
        return attention_map

    def visualize_morphogen_gradients(self, model_path=None, resolution=50, num_gradients=3, output_file=None):
        """
        Visualize how the encoder represents synthetic morphogen gradients.
        
        Args:
            model_path: Path to the trained encoder model weights
            resolution: Resolution of the grid
            num_gradients: Number of morphogen gradients to visualize
            output_file: Path to save the visualization
        """
        # Load model if provided
        if model_path:
            print(f"Loading trained model from {model_path}")
            try:
                state_dict = torch.load(model_path)
                self.encoder.load_state_dict(state_dict)
                print("Model loaded successfully!")
            except Exception as e:
                print(f"Error loading model: {e}")
                print("Proceeding with untrained model...")
        
        # Set to evaluation mode
        self.encoder.eval()
        
        # Create a grid of positions
        x = np.linspace(0, self.max_spatial_distance, resolution)
        y = np.linspace(0, self.max_spatial_distance, resolution)
        xx, yy = np.meshgrid(x, y)
        positions = np.column_stack([xx.flatten(), yy.flatten()])
        num_positions = len(positions)
        
        # Convert to tensor
        positions_tensor = torch.tensor(positions, dtype=torch.float32).unsqueeze(0)
        
        # Fixed time (middle of range)
        time_tensor = torch.ones(1, num_positions, 1) * (self.max_time / 2)
        
        # Generate synthetic morphogen gradients
        gradient_values = np.zeros((num_gradients, num_positions))
        gradient_names = []
        
        for i in range(num_gradients):
            # Create different types of morphogen gradients
            if i == 0:
                # Gradient from left to right
                gradient_values[i] = positions[:, 0] / self.max_spatial_distance
                gradient_names.append("Left-to-Right")
            elif i == 1:
                # Radial gradient from center
                center = np.array([self.max_spatial_distance/2, self.max_spatial_distance/2])
                distances = np.sqrt(np.sum((positions - center) ** 2, axis=1))
                max_dist = np.sqrt(2) * self.max_spatial_distance / 2
                gradient_values[i] = 1 - (distances / max_dist)
                gradient_names.append("Radial from Center")
            elif i == 2:
                # Exponential decay from source point
                source = np.array([20, 20])
                distances = np.sqrt(np.sum((positions - source) ** 2, axis=1))
                gradient_values[i] = np.exp(-distances / 30)
                gradient_names.append("Exponential from Source")
            else:
                # Random combination of position components
                gradient_values[i] = 0.7 * (positions[:, 0] / self.max_spatial_distance) + \
                                0.3 * (1 - positions[:, 1] / self.max_spatial_distance)
                gradient_names.append(f"Combined Gradient {i}")
        
        # Create baseline features with gradient values
        all_encodings = []
        
        for i in range(num_gradients):
            # Set up features with this gradient embedded
            cell_features = torch.zeros(1, num_positions, self.embedding_dim)
            
            # Embed the gradient in the first few dimensions
            gradient_tensor = torch.tensor(gradient_values[i], dtype=torch.float32).unsqueeze(0).unsqueeze(2)
            gradient_tensor = gradient_tensor.expand(1, num_positions, min(4, self.embedding_dim))
            cell_features[:, :, :gradient_tensor.shape[2]] = gradient_tensor
            
            # Generate encodings
            with torch.no_grad():
                encodings = self.encoder(positions_tensor, time_tensor, cell_features)
                all_encodings.append(encodings.squeeze(0).numpy())
        
        # Perform PCA on encodings
        all_encodings_flat = np.vstack([enc for enc in all_encodings])
        pca = PCA(n_components=3)
        pca_result = pca.fit_transform(all_encodings_flat)
        
        # Split back into separate gradients
        pca_by_gradient = []
        for i in range(num_gradients):
            start_idx = i * num_positions
            end_idx = start_idx + num_positions
            pca_by_gradient.append(pca_result[start_idx:end_idx])
        
        # Create figure
        fig = plt.figure(figsize=(15, 5 * num_gradients))
        
        for i in range(num_gradients):
            # Original gradient
            ax1 = fig.add_subplot(num_gradients, 3, i*3 + 1)
            gradient_reshaped = gradient_values[i].reshape(resolution, resolution)
            im1 = ax1.imshow(gradient_reshaped, extent=[0, self.max_spatial_distance, 0, self.max_spatial_distance], 
                        origin='lower', cmap='viridis')
            plt.colorbar(im1, ax=ax1)
            ax1.set_title(f'Original {gradient_names[i]} Gradient')
            ax1.set_xlabel('X Position')
            ax1.set_ylabel('Y Position')
            
            # First two PCA components
            ax2 = fig.add_subplot(num_gradients, 3, i*3 + 2)
            pca_component1 = pca_by_gradient[i][:, 0].reshape(resolution, resolution)
            im2 = ax2.imshow(pca_component1, extent=[0, self.max_spatial_distance, 0, self.max_spatial_distance], 
                        origin='lower', cmap='viridis')
            plt.colorbar(im2, ax=ax2)
            ax2.set_title(f'Encoded {gradient_names[i]} (PC1)')
            ax2.set_xlabel('X Position')
            ax2.set_ylabel('Y Position')
            
            # 3D visualization of first three components
            ax3 = fig.add_subplot(num_gradients, 3, i*3 + 3, projection='3d')
            sc = ax3.scatter(pca_by_gradient[i][:, 0], pca_by_gradient[i][:, 1], pca_by_gradient[i][:, 2],
                        c=gradient_values[i], cmap='viridis', s=5, alpha=0.7)
            plt.colorbar(sc, ax=ax3)
            ax3.set_title(f'Encoded {gradient_names[i]} (3D)')
            ax3.set_xlabel('PC1')
            ax3.set_ylabel('PC2')
            ax3.set_zlabel('PC3')
        
        plt.tight_layout()
        
        # Save figure
        if output_file is None:
            output_file = os.path.join(self.output_dir, "morphogen_gradients.png")
        plt.savefig(output_file, dpi=300)
        plt.close()
        
        print(f"Morphogen gradient visualization saved to {output_file}")
        
        return gradient_values, pca_by_gradient

    def visualize_dimensionality_analysis(self, model_path=None, patterns=['grid', 'spiral', 'random', 'clusters'], output_file=None):
        """
        Analyze the effective dimensionality of the encoding space for different patterns.
        
        Args:
            model_path: Path to the trained encoder model weights
            patterns: List of patterns to analyze
            output_file: Path to save the visualization
        """
        # Load model if provided
        if model_path:
            print(f"Loading trained model from {model_path}")
            try:
                state_dict = torch.load(model_path)
                self.encoder.load_state_dict(state_dict)
                print("Model loaded successfully!")
            except Exception as e:
                print(f"Error loading model: {e}")
                print("Proceeding with untrained model...")
        
        # Set to evaluation mode
        self.encoder.eval()
        
        # Generate data and compute encodings for each pattern
        pattern_encodings = {}
        pattern_pca_results = {}
        pattern_effective_dims = {}
        
        for pattern in patterns:
            # Generate pattern data
            results = self.generate_pattern(
                pattern_type=pattern,
                num_cells=100,
                time_points=[0, 25, 50, 75, 100]
            )
            
            # Collect all encodings for this pattern
            all_encodings = torch.cat(results['encodings'], dim=1).squeeze(0).numpy()
            
            # Store encodings
            pattern_encodings[pattern] = all_encodings
            
            # Perform PCA
            pca = PCA()
            pca_result = pca.fit_transform(all_encodings)
            
            # Store PCA results
            pattern_pca_results[pattern] = pca_result
            
            # Calculate effective dimensionality
            explained_variance = pca.explained_variance_ratio_
            cumulative_variance = np.cumsum(explained_variance)
            
            # Number of dimensions needed to explain 90% of variance
            dims_90 = np.argmax(cumulative_variance >= 0.9) + 1
            
            # Number of dimensions needed to explain 99% of variance
            dims_99 = np.argmax(cumulative_variance >= 0.99) + 1
            
            # Participation ratio (measure of effective dimensionality)
            participation_ratio = 1.0 / np.sum(explained_variance**2)
            
            pattern_effective_dims[pattern] = {
                'dims_90': dims_90,
                'dims_99': dims_99,
                'participation_ratio': participation_ratio,
                'explained_variance': explained_variance
            }
        
        # Create figure
        plt.figure(figsize=(15, 10))
        
        # Plot explained variance for each pattern
        ax1 = plt.subplot(2, 2, 1)
        for pattern in patterns:
            variance = pattern_effective_dims[pattern]['explained_variance']
            plt.plot(np.arange(1, len(variance) + 1), variance, 'o-', label=pattern)
        
        plt.title('Explained Variance by Principal Component')
        plt.xlabel('Principal Component')
        plt.ylabel('Explained Variance Ratio')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.yscale('log')
        
        # Plot cumulative variance for each pattern
        ax2 = plt.subplot(2, 2, 2)
        for pattern in patterns:
            variance = pattern_effective_dims[pattern]['explained_variance']
            cumulative = np.cumsum(variance)
            plt.plot(np.arange(1, len(cumulative) + 1), cumulative, 'o-', label=pattern)
        
        plt.axhline(y=0.9, color='r', linestyle='--', alpha=0.5, label='90% Threshold')
        plt.axhline(y=0.99, color='g', linestyle='--', alpha=0.5, label='99% Threshold')
        plt.title('Cumulative Explained Variance')
        plt.xlabel('Number of Components')
        plt.ylabel('Cumulative Explained Variance')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # Bar chart of effective dimensionality
        ax3 = plt.subplot(2, 2, 3)
        x = np.arange(len(patterns))
        width = 0.35
        
        dims_90 = [pattern_effective_dims[p]['dims_90'] for p in patterns]
        dims_99 = [pattern_effective_dims[p]['dims_99'] for p in patterns]
        
        ax3.bar(x - width/2, dims_90, width, label='90% Variance')
        ax3.bar(x + width/2, dims_99, width, label='99% Variance')
        
        ax3.set_title('Effective Dimensionality by Pattern')
        ax3.set_xticks(x)
        ax3.set_xticklabels(patterns)
        ax3.set_ylabel('Number of Dimensions')
        ax3.legend()
        
        # Participation ratio comparison
        ax4 = plt.subplot(2, 2, 4)
        participation_ratios = [pattern_effective_dims[p]['participation_ratio'] for p in patterns]
        
        ax4.bar(patterns, participation_ratios)
        ax4.set_title('Participation Ratio by Pattern')
        ax4.set_ylabel('Participation Ratio')
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # Save figure
        if output_file is None:
            output_file = os.path.join(self.output_dir, "dimensionality_analysis.png")
        plt.savefig(output_file, dpi=300)
        plt.close()
        
        # Save detailed metrics to a text file
        metrics_file = os.path.join(self.output_dir, "dimensionality_metrics.txt")
        with open(metrics_file, 'w') as f:
            f.write("DIMENSIONALITY ANALYSIS METRICS\n")
            f.write("===============================\n\n")
            
            for pattern in patterns:
                f.write(f"Pattern: {pattern.upper()}\n")
                f.write(f"  Dimensions for 90% variance: {pattern_effective_dims[pattern]['dims_90']}\n")
                f.write(f"  Dimensions for 99% variance: {pattern_effective_dims[pattern]['dims_99']}\n")
                f.write(f"  Participation ratio: {pattern_effective_dims[pattern]['participation_ratio']:.4f}\n")
                f.write(f"  Top 5 components explain: {np.sum(pattern_effective_dims[pattern]['explained_variance'][:5]) * 100:.2f}% of variance\n")
                f.write("\n")
        
        print(f"Dimensionality analysis visualization saved to {output_file}")
        print(f"Detailed metrics saved to {metrics_file}")
        
        return pattern_effective_dims

# Example usage
if __name__ == "__main__":
    import argparse
    
    # Create argument parser
    parser = argparse.ArgumentParser(description='Visualize spatiotemporal encodings')
    parser.add_argument('--model_path', type=str, default=None, 
                        help='Path to trained encoder model weights')
    parser.add_argument('--output_dir', type=str, default="encoding_visualizations", 
                        help='Directory to save visualizations')
    parser.add_argument('--skip_standard', action='store_true',
                        help='Skip standard pattern visualizations')
    parser.add_argument('--verifications_only', action='store_true',
                        help='Run only verification visualizations')
    
    args = parser.parse_args()
    
    # Create visualizer with model path
    visualizer = SpatiotemporalVisualizer(
        embedding_dim=64,
        max_spatial_distance=100.0,
        max_time=100,
        output_dir=args.output_dir
    )
    
    if not args.verifications_only:
        # Generate and visualize different patterns
        patterns = ['grid', 'spiral', 'random', 'clusters']
        
        for pattern in patterns:
            # Generate pattern data
            results = visualizer.generate_pattern(
                pattern_type=pattern,
                num_cells=100,
                time_points=[0, 25, 50, 75, 100],
                grid_size=10,
                spiral_loops=3.0,
                cluster_centers=5,
                cluster_std=10.0
            )
            
            # Visualize encoding space
            visualizer.visualize_encoding_space(results, pattern, plot_type='2d')
            visualizer.visualize_encoding_space(results, pattern, plot_type='3d')
            
            # Create animation
            visualizer.create_animation(results, pattern)
        
        # Generate unified morphospace
        visualizer.generate_morphospace()
    
    # Run verification visualizations
    print("\nRunning verification visualizations...")
    
    # 1. Cell trajectory analysis
    visualizer.visualize_trajectory_analysis(model_path=args.model_path)
    
    # 2. Boundary response test
    visualizer.visualize_boundary_response(model_path=args.model_path)
    
    # 3. Temporal oscillation analysis
    visualizer.visualize_temporal_oscillations(model_path=args.model_path)
    
    # 4. Ablation comparison (trained vs baseline)
    if args.model_path:
        visualizer.visualize_ablation_comparison(args.model_path)
    else:
        print("Skipping ablation comparison - no trained model provided")

    print("\nRunning additional verification visualizations...")

    # 5. Attention heatmap visualization
    visualizer.visualize_attention_heatmap(model_path=args.model_path)

    # 6. Morphogen gradient response
    visualizer.visualize_morphogen_gradients(model_path=args.model_path)

    # 7. Dimensionality analysis
    visualizer.visualize_dimensionality_analysis(model_path=args.model_path)
    
    print("\nAll visualizations created successfully!")
